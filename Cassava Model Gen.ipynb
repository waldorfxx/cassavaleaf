{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import cv2 \n",
    "import numba.cuda as cuda\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "import timm\n",
    "\n",
    "from optimizers import SAM, Lookahead, Ralamb, AdamP\n",
    "from losses import (bi_tempered_logistic_loss, SmoothCrossEntropyLoss, FocalCosineLoss,\n",
    "                    SymmetricCrossEntropy, TaylorCrossEntropyLoss)\n",
    "from augmentations import snapmix, SnapMixLoss, RandomAugMix, GridMask\n",
    "\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG :\n",
    "    time = datetime.datetime.now().strftime(format='%Y%m%d_%H%M%S')\n",
    "    debug = False\n",
    "    \n",
    "    epochs = 40\n",
    "    batch_size = 64\n",
    "    device = 'cuda:0'\n",
    "    verbose = 2\n",
    "    seed = 666\n",
    "    n_fold = 5\n",
    "    \n",
    "    img_size = 586\n",
    "    crop_ratio = 0.875\n",
    "    crop_size = int(img_size*crop_ratio)\n",
    "    classes = [0,1,2,3,4]\n",
    "    train_cache = True\n",
    "    valid_cache = True\n",
    "    num_workers = 7\n",
    "    \n",
    "    snapmix_pct = 0.5\n",
    "    snapmix_alpha = 5\n",
    "    gridmask_pct = 0.25\n",
    "    gridmask_num = 3\n",
    "    gridmask_mode = 3\n",
    "    augmix_pct = 0.5\n",
    "    augmix_severity = 2\n",
    "    augmix_width = 3\n",
    "    augmix_alpha = 0.1\n",
    "    \n",
    "    model = 'tf_efficientnet_b1_ns'\n",
    "    \n",
    "    optimizer = 'adamw'\n",
    "    lr = 1e-04\n",
    "    momentum = 0.9\n",
    "    eps = 1e-08\n",
    "    betas = (0.9, 0.99)\n",
    "    weight_decay = 1e-6\n",
    "    amsgrad = True\n",
    "    lookahead = False\n",
    "    swa = False\n",
    "    \n",
    "    # LOSS\n",
    "    # cross_entropy, bi_tempered_logistic, smooth_cross_entropy\n",
    "    # focal_cosine, symmetric_cross_entropy, taylor_cross_entropy\n",
    "    loss = 'bi_tempered_logistic'\n",
    "    t0 = 0.8\n",
    "    t1 = 1.2\n",
    "    label_smoothing=0.2\n",
    "    \n",
    "    scheduler = 'cosine'\n",
    "    cos_t0 = 10\n",
    "    min_lr = 1e-6\n",
    "    \n",
    "    early_stop_epochs = epochs\n",
    "    save_best = True\n",
    "    \n",
    "CFG.crop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_IMG_PATH = '../input/cassava-leaf-disease-classification/train_images'\n",
    "# TRAIN_DF = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "TRAIN_IMG_PATH = '../input/cassava-leaf-disease-merged/train_images'\n",
    "TRAIN_DF = pd.read_csv('../input/cassava-leaf-disease-merged/train.csv')\n",
    "TRAIN_DF['path'] = [os.path.join(TRAIN_IMG_PATH, fn) for fn in TRAIN_DF.image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug :\n",
    "    CFG.epochs = 1\n",
    "    CFG.n_fold = 3\n",
    "    CFG.save_best = False\n",
    "    TRAIN_DF = TRAIN_DF.iloc[:int(0.05*len(TRAIN_DF))]\n",
    "    print('Debug Mode Activated', len(TRAIN_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels,\n",
    "                 transform=None, cache_ds=False, use_cache=False):\n",
    "        self.file_paths = list(file_paths)\n",
    "        self.labels = list(labels)\n",
    "        self.transform = transform\n",
    "        self.cached_images = []\n",
    "        self.cached_labels = []\n",
    "        self.cache_ds = cache_ds\n",
    "        self.use_cache = use_cache\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.use_cache or self.cache_ds :\n",
    "            label = self.labels[idx]\n",
    "            file_path = self.file_paths[idx]\n",
    "\n",
    "            # Read an image with OpenCV\n",
    "            image = cv2.imread(file_path)\n",
    "\n",
    "            # By default OpenCV uses BGR color space for color images,\n",
    "            # so we need to convert the image to RGB color space.\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             image = cv2.resize(image, dsize=(CFG.img_size, CFG.img_size), \n",
    "#                                interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            if not self.cache_ds :\n",
    "                self.cached_images.append(image)\n",
    "                self.cached_labels.append(label)\n",
    "        \n",
    "        else :\n",
    "            image = self.cached_images[idx]\n",
    "            label = self.cached_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image) \n",
    "            image = augmented['image']\n",
    "                \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(CFG.crop_size, CFG.crop_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=1),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    \n",
    "    RandomAugMix(\n",
    "        severity=CFG.augmix_severity,\n",
    "        width=CFG.augmix_width,\n",
    "        alpha=CFG.augmix_alpha,\n",
    "        p=CFG.augmix_pct),\n",
    "    \n",
    "    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "    \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "    A.CoarseDropout(p=0.5),\n",
    "#     A.Cutout(p=0.5),\n",
    "    A.OneOf([\n",
    "        GridMask(num_grid=CFG.gridmask_num,\n",
    "                 mode=CFG.gridmask_mode, \n",
    "                 rotate=15),\n",
    "        GridMask(num_grid=CFG.gridmask_num,\n",
    "                 mode=CFG.gridmask_mode,\n",
    "                 rotate=15),\n",
    "    ], p=CFG.gridmask_pct),\n",
    "    A.pytorch.ToTensor(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "#     A.CenterCrop(CFG.crop_size, CFG.crop_size, p=1.),\n",
    "    A.RandomResizedCrop(CFG.crop_size, CFG.crop_size),\n",
    "    A.Resize(CFG.crop_size, CFG.crop_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "    A.pytorch.ToTensor(),\n",
    "])\n",
    "\n",
    "inference_transform = A.Compose([\n",
    "    A.RandomResizedCrop(CFG.crop_size, CFG.crop_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=1),\n",
    "    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "    A.pytorch.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CassavaDataset(\n",
    "#     file_paths=TRAIN_DF.path,\n",
    "#     labels=TRAIN_DF.label,\n",
    "#     transform=train_transform,\n",
    "# )\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=CFG.batch_size,\n",
    "#     shuffle=True, \n",
    "#     num_workers=0\n",
    "# )\n",
    "\n",
    "# num_samples = 8\n",
    "\n",
    "# def imshow(img):\n",
    "# #     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # get some random training images\n",
    "# dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# # show images\n",
    "# plt.figure(figsize=(20,10))\n",
    "# imshow(torchvision.utils.make_grid(images[:num_samples], nrow=4))\n",
    "# print(labels[:num_samples])\n",
    "\n",
    "# del num_samples, images, labels, dataiter\n",
    "# del train_dataset, train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, Loss, Optimizer\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # https://www.kaggle.com/sachinprabhu/pytorch-resnet50-snapmix-train-pipeline\n",
    "    def __init__(self, model_arch, n_class, pretrained=False) :\n",
    "        super().__init__()\n",
    "        backbone = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        if 'rexnet' in model_arch or 'regnet' in model_arch :\n",
    "            last_layer = list(backbone.children())[-1]\n",
    "            n_features = last_layer.fc.in_features\n",
    "            self.backbone = nn.Sequential(*backbone.children())[:-1]\n",
    "        elif 'dpn' in model_arch or 'dla' in model_arch :\n",
    "            last_layer = list(backbone.children())[-1]\n",
    "            n_features = last_layer.in_channels\n",
    "            self.backbone = nn.Sequential(*backbone.children())[:-1]\n",
    "        else :\n",
    "            last_layer = list(backbone.children())[-1]\n",
    "            n_features = last_layer.in_features\n",
    "            self.backbone = nn.Sequential(*backbone.children())[:-2]\n",
    "        self.classifier = nn.Linear(n_features, n_class)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "    def forward_features(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.forward_features(x)\n",
    "        x = self.pool(feats).view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x, feats\n",
    "  \n",
    "# dpn68b\n",
    "# dla60_res2next\n",
    "# dla60x\n",
    "# Net('dla60_res2next', len(CFG.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOSS \"\"\"\n",
    "def set_loss() :\n",
    "    if CFG.loss == 'cross_entropy' :\n",
    "        criterion = nn.CrossEntropyLoss().to(CFG.device)\n",
    "    elif CFG.loss == 'bi_tempered_logistic' :\n",
    "        def bi_tempered_logistic_loss_fn(outputs, labels) :\n",
    "            return bi_tempered_logistic_loss(outputs, labels,\\\n",
    "                        CFG.t0, CFG.t1, label_smoothing=CFG.label_smoothing)\n",
    "        criterion = bi_tempered_logistic_loss_fn\n",
    "    elif CFG.loss == 'smooth_cross_entropy' :\n",
    "        criterion = SmoothCrossEntropyLoss(smoothing=CFG.label_smoothing).to(CFG.device)\n",
    "    elif CFG.loss == 'focal_cosine' :\n",
    "        criterion = FocalCosineLoss().to(CFG.device)\n",
    "    elif CFG.loss == 'symmetric_cross_entropy' :\n",
    "        criterion = SymmetricCrossEntropy(num_classes=len(CFG.classes)).to(CFG.device)\n",
    "    elif CFG.loss == 'taylor_cross_entropy' :\n",
    "        criterion = TaylorCrossEntropyLoss().to(CFG.device)\n",
    "    else : \n",
    "        criterion = None\n",
    "        \n",
    "    assert criterion is not None\n",
    "    return criterion\n",
    "\n",
    "\"\"\" Optimizers \"\"\"\n",
    "def set_optimizer(model) :\n",
    "    if CFG.optimizer == 'rmsprop' :\n",
    "        optimizer = optim.RMSprop(model.parameters(), \n",
    "                                  lr=CFG.lr, momentum=CFG.momentum)\n",
    "    elif CFG.optimizer == 'sgd' :\n",
    "        optimizer = optim.SGD(model.parameters(), \n",
    "                              lr=CFG.lr, momentum=CFG.momentum)\n",
    "    elif CFG.optimizer == 'adam' :\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "    elif CFG.optimizer == 'adamw' :\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                                lr=CFG.lr, betas=CFG.betas,\n",
    "                               eps=CFG.eps, weight_decay=CFG.weight_decay, \n",
    "                                amsgrad=CFG.amsgrad)\n",
    "    elif CFG.optimizer == 'ralamb' :\n",
    "        optimizer = Ralamb(model.parameters(), lr=CFG.lr, \n",
    "                           weight_decay=CFG.weight_decay)\n",
    "    elif CFG.optimizer == 'sam' :\n",
    "        optimizer = SAM(model.parameters(), optim.SGD,\n",
    "                        lr=CFG.lr, momentum=CFG.momentum)\n",
    "    elif CFG.optimizer == 'adamp' :\n",
    "        optimizer = AdamP(model.parameters(), \n",
    "                          lr=CFG.lr, betas=CFG.betas,\n",
    "                          weight_decay=CFG.weight_decay)\n",
    "    else :\n",
    "        optimizer = None\n",
    "        \n",
    "    if CFG.lookahead :\n",
    "        optimizer = Lookahead(optimizer)\n",
    "        \n",
    "    if CFG.swa :\n",
    "        optimizer = optim.swa_utils.SWALR(optimizer, anneal_epochs=10, swa_freq=2, swa_lr=CFG.min_lr, ) ## SWA\n",
    "    \n",
    "    assert optimizer is not None\n",
    "    return optimizer\n",
    "\n",
    "\"\"\" Scheduler \"\"\"\n",
    "def set_scheduler(optimizer) :\n",
    "    if CFG.scheduler == 'cosine' :\n",
    "#         scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, 2, eta_min=1e-6) # 1e-6  ### Cosine Warm \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                   T_0=CFG.cos_t0, T_mult=1,\n",
    "                                                                   eta_min=CFG.min_lr, last_epoch=-1)\n",
    "    elif CFG.scheduler == 'steplr' :\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                              step_size=5,\n",
    "                                              gamma=0.5),\n",
    "    else :\n",
    "        scheduler = None\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture cap --no-stderr\n",
    "################ Generate Dataset & Dataloader ###########################\n",
    "train_set = TRAIN_DF.reset_index(drop=True)\n",
    "\n",
    "train_dataset = CassavaDataset(\n",
    "    file_paths=train_set.path,\n",
    "    labels=train_set.label,\n",
    "    transform=train_transform,\n",
    "    cache_ds=CFG.train_cache\n",
    ")\n",
    "\n",
    "if not CFG.train_cache :\n",
    "    train_loader.num_workers = CFG.num_workers\n",
    "\n",
    "models = []; batches=[]\n",
    "\n",
    "# models += [f'tf_efficientnet_b{i}_ns' for i in range(5)]\n",
    "# batches += [128, 96, 80, 64, 48]\n",
    "\n",
    "# models += ['mixnet_l', 'mixnet_xl']\n",
    "# batches += [64, 48]\n",
    "\n",
    "# models += ['rexnet_130', 'rexnet_150', 'rexnet_200']\n",
    "# batches += [80, 72, 56]\n",
    "\n",
    "# models += ['regnetx_032', 'regnety_032']\n",
    "# batches += [112, 96]\n",
    "\n",
    "models += ['dpn68b','dla60_res2next','dla60x']\n",
    "batches += [96, 64, 80]\n",
    "\n",
    "for m, mod in enumerate(models) :\n",
    "#     if m < 8 : continue\n",
    "    CFG.model = mod\n",
    "    CFG.batch_size = batches[m]\n",
    "    filename = f'{CFG.model}_{CFG.seed}'\n",
    "    print(filename, CFG.batch_size)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    ################# Set Model, Loss, Optimizer, Scaler, Scheduler ###########################\n",
    "    model = Net(CFG.model, len(CFG.classes), pretrained=True).to(CFG.device)\n",
    "    criterion = set_loss()\n",
    "    snapmix_criterion = SnapMixLoss().to(CFG.device)\n",
    "    optimizer = set_optimizer(model)\n",
    "    scaler = GradScaler()\n",
    "    scheduler = set_scheduler(optimizer)\n",
    "    \n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_start = 5\n",
    "    swa_scheduler = SWALR(optimizer, swa_lr=CFG.lr)\n",
    "\n",
    "    cached = False\n",
    "    \n",
    "    ## Early Stopping\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for j, epoch in enumerate(range(CFG.epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "        ############## TRAIN ####################\n",
    "        stime = time.time()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for data in train_loader :\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(CFG.device)\n",
    "            labels = labels.to(CFG.device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            with autocast() :\n",
    "                rand = np.random.rand()\n",
    "                if rand > (1.0-CFG.snapmix_pct):\n",
    "                    inputs, ya, yb, lam_a, lam_b = snapmix(inputs, labels, CFG.snapmix_alpha, model)\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = snapmix_criterion(criterion, outputs, ya, yb, lam_a, lam_b)\n",
    "                else:\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "    \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        ## Scheduler\n",
    "        if epoch > swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        train_loss = running_loss/len(train_loader)\n",
    "        train_acc = correct/total*100\n",
    "        \n",
    "        training_time = time.time()-stime\n",
    "        print(f'[EPOCH {epoch+1}/{CFG.epochs}] time: {training_time:.2f}sec -',\n",
    "              f'loss: {train_loss:.4E} - acc: {train_acc:.2f}%')\n",
    "\n",
    "        if not cached and (CFG.train_cache or CFG.valid_cache) :\n",
    "            if CFG.train_cache :\n",
    "                train_loader.dataset.set_use_cache(use_cache=True)\n",
    "                train_loader.num_workers = CFG.num_workers\n",
    "            cached = True\n",
    "    \n",
    "#     info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "#     print(info.used//1024//1024)\n",
    "    \n",
    "    torch.save(swa_model.state_dict(), f'../output/{filename}.pt')        \n",
    "    del model, swa_model, optimizer, scaler, scheduler, swa_scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
